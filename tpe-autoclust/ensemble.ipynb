{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load datasets (assuming each CSV contains a single column of cluster labels)\n",
    "cv_names = ['sil', 'dbs', 'chs']\n",
    "dataset_names = ['ecoli_labels',\n",
    " 'pathbased_labels',\n",
    " 'thy_labels',\n",
    " 'iris_labels',\n",
    " 'jain_labels',\n",
    " 'arrhythmia_labels',\n",
    " 'compound_labels',\n",
    " 'iono_labels',\n",
    " 'sizes4_labels',\n",
    " '3-spiral_labels',\n",
    " 'cluto-t7-10k_labels',\n",
    " 'sonar_labels',\n",
    " 'glass_labels',\n",
    " 'tae_labels',\n",
    " 'segment_labels',\n",
    " 'sizes2_labels',\n",
    " 'balance-scale_labels',\n",
    " 'cassini_labels',\n",
    " 'elliptical_10_2_labels',\n",
    " 'engytime_labels',\n",
    " 'flame_labels',\n",
    " 'fourty_labels',\n",
    " 'twodiamonds_labels',\n",
    " 'wine_labels',\n",
    " 'disk-6000n_labels']\n",
    "\n",
    "filepaths = [f'../results/{cvi}/labels/{dataset}.csv' for dataset in dataset_names for cvi in cv_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ClusterEnsembles as CE\n",
    "\n",
    "# Process each dataset and generate the final clustering result\n",
    "for dataset_name in dataset_names:\n",
    "    # Get the file paths for the current dataset (3 file paths for each CV method)\n",
    "    dataset_filepaths = filepaths[dataset_names.index(dataset_name)*len(cv_names):(dataset_names.index(dataset_name)+1)*len(cv_names)]\n",
    "\n",
    "    # Load the three label files for the current dataset\n",
    "    datasets = [pd.read_csv(f).iloc[:, 0].values for f in dataset_filepaths]\n",
    "    datasets = np.array(datasets)\n",
    "    final_labels = CE.cluster_ensembles(datasets)\n",
    "    final_filename = f'final_ensemble_clustering_{dataset_name}.csv'\n",
    "    pd.DataFrame(final_labels, columns=['Cluster']).to_csv(f\"results/ensemble/labels/{final_filename}\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: cluto-t7-10k.csv\n",
      "Completed cluto-t7-10k.csv with ARI: 0.222871433459034\n",
      "\n",
      "Processing dataset: segment.csv\n",
      "Completed segment.csv with ARI: 0.355381067671688\n",
      "\n",
      "Processing dataset: twodiamonds.csv\n",
      "Completed twodiamonds.csv with ARI: 0.08792079850107448\n",
      "\n",
      "Processing dataset: fourty.csv\n",
      "Completed fourty.csv with ARI: 0.23588052650089633\n",
      "\n",
      "Processing dataset: disk-6000n.csv\n",
      "Completed disk-6000n.csv with ARI: 0.07761973232853507\n",
      "\n",
      "Processing dataset: wine.csv\n",
      "Completed wine.csv with ARI: 0.08883855949054086\n",
      "\n",
      "Processing dataset: balance-scale.csv\n",
      "Completed balance-scale.csv with ARI: 0.07490040140879939\n",
      "\n",
      "Processing dataset: tae.csv\n",
      "Completed tae.csv with ARI: 0.017493618430659854\n",
      "\n",
      "Processing dataset: cassini.csv\n",
      "Completed cassini.csv with ARI: 0.6395557917623784\n",
      "\n",
      "Processing dataset: pathbased.csv\n",
      "Completed pathbased.csv with ARI: 0.11844729302892097\n",
      "\n",
      "Processing dataset: iono.csv\n",
      "Completed iono.csv with ARI: 0.02387477921919727\n",
      "\n",
      "Processing dataset: flame.csv\n",
      "Completed flame.csv with ARI: 0.06621482592234162\n",
      "\n",
      "Processing dataset: glass.csv\n",
      "Completed glass.csv with ARI: 0.07202080875442646\n",
      "\n",
      "Processing dataset: sizes2.csv\n",
      "Completed sizes2.csv with ARI: 0.44896067670614226\n",
      "\n",
      "Processing dataset: iris.csv\n",
      "Completed iris.csv with ARI: 0.1312242042672263\n",
      "\n",
      "Processing dataset: engytime.csv\n",
      "Completed engytime.csv with ARI: 0.32841368606597715\n",
      "\n",
      "Processing dataset: elliptical_10_2.csv\n",
      "Completed elliptical_10_2.csv with ARI: 0.13434985842002461\n",
      "\n",
      "Processing dataset: sonar.csv\n",
      "Completed sonar.csv with ARI: 0.02139983758210117\n",
      "\n",
      "Processing dataset: jain.csv\n",
      "Completed jain.csv with ARI: 0.050090445948163975\n",
      "\n",
      "Processing dataset: sizes4.csv\n",
      "Completed sizes4.csv with ARI: 0.0635619130497606\n",
      "\n",
      "Processing dataset: 3-spiral.csv\n",
      "Completed 3-spiral.csv with ARI: 0.03169350280202839\n",
      "\n",
      "Processing dataset: compound.csv\n",
      "Completed compound.csv with ARI: 0.3958715327415243\n",
      "\n",
      "Processing dataset: thy.csv\n",
      "Completed thy.csv with ARI: 0.04219093123259214\n",
      "\n",
      "Processing dataset: arrhythmia.csv\n",
      "Completed arrhythmia.csv with ARI: 0.0017346518518287328\n",
      "\n",
      "Processing dataset: ecoli.csv\n",
      "Completed ecoli.csv with ARI: 0.12971455261583745\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tpot import TPOTClustering\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "scoring_metric = \"ensemble\"\n",
    "validation_folder = \"/home/camilo/dev/ijcnn_25/datasets/validation_csv\"\n",
    "results_file = f\"results/{scoring_metric}/tpe_autoclust_results.csv\"\n",
    "output_folder = f\"results/{scoring_metric}\"\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def plot_pca_comparison(X, y, labels, ari_score, save_path):\n",
    "    \"\"\"Generate and save PCA plots comparing original labels and cluster labels.\"\"\"\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "    axes[0].set_title('PCA with Original Labels')\n",
    "    axes[0].legend(*scatter1.legend_elements(), title=\"Classes\")\n",
    "\n",
    "    scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='plasma', edgecolor='k')\n",
    "    axes[1].set_title(f'PCA with Cluster Labels\\nARI: {ari_score:.2f}')\n",
    "    axes[1].legend(*scatter2.legend_elements(), title=\"Clusters\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def get_processed_datasets(results_file):\n",
    "    \"\"\"Load processed datasets from results file.\"\"\"\n",
    "    if os.path.exists(results_file):\n",
    "        df = pd.read_csv(results_file)\n",
    "        return set(df['Dataset'])\n",
    "    return set()\n",
    "\n",
    "# Initialize the results file if it doesn't exist\n",
    "if not os.path.exists(results_file):\n",
    "    pd.DataFrame(columns=[\"Dataset\", \"silhouette_score\", \n",
    "                          \"davies_bouldin_score\", \"calinski_harabasz_score\", \n",
    "                          \"adjusted_rand_score\", \"Running_Time(s)\"]\n",
    "                ).to_csv(results_file, index=False)\n",
    "\n",
    "# Get the list of already processed datasets\n",
    "processed_datasets = get_processed_datasets(results_file)\n",
    "\n",
    "# Iterate over datasets\n",
    "for dataset_name in os.listdir(validation_folder):\n",
    "    if not dataset_name.endswith(\".csv\") or dataset_name in processed_datasets:\n",
    "        print(f\"Skipping {dataset_name}, already processed.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Load and preprocess data\n",
    "        df = pd.read_csv(os.path.join(validation_folder, dataset_name))\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "        # Convert categorical labels to numeric if necessary\n",
    "        if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "            y = y.astype('category').cat.codes.to_numpy()\n",
    "        else:\n",
    "            y = y.to_numpy()\n",
    "\n",
    "        # Scale numeric features\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(X.select_dtypes(include=['number']))\n",
    "        \n",
    "        labels = pd.read_csv(f\"results/ensemble/labels/final_ensemble_clustering_{dataset_name.replace('.csv','')}_labels.csv\", header=None)\n",
    "        labels = labels.values.ravel()\n",
    "\n",
    "        # Calculate metrics\n",
    "        sil = silhouette_score(X_scaled, labels)\n",
    "        dbs = davies_bouldin_score(X_scaled, labels)\n",
    "        chs = calinski_harabasz_score(X_scaled, labels)\n",
    "        ari = adjusted_rand_score(y, labels)\n",
    "        running_time = round(time.time() - start_time, 2)\n",
    "\n",
    "        # Save results\n",
    "        results = pd.DataFrame({\n",
    "            \"Dataset\": [dataset_name],\n",
    "            \"silhouette_score\": [sil],\n",
    "            \"davies_bouldin_score\": [dbs],\n",
    "            \"calinski_harabasz_score\": [chs],\n",
    "            \"adjusted_rand_score\": [ari],\n",
    "            \"Running_Time(s)\": [running_time]\n",
    "        })\n",
    "\n",
    "        results.to_csv(results_file, mode=\"a\", header=False, index=False)\n",
    "\n",
    "        # Save PCA plot\n",
    "        plot_pca_comparison(X_scaled, y, labels, ari, f\"{output_folder}/{dataset_name.replace('.csv', '_pca.png')}\")\n",
    "        print(f\"Completed {dataset_name} with ARI: {ari}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dataset_name}: {e}\")\n",
    "        error_results = pd.DataFrame({\n",
    "            \"Dataset\": [dataset_name],\n",
    "            \"silhouette_score\": [\"ERROR\"],\n",
    "            \"davies_bouldin_score\": [\"ERROR\"],\n",
    "            \"calinski_harabasz_score\": [\"ERROR\"],\n",
    "            \"adjusted_rand_score\": [\"ERROR\"],\n",
    "            \"Running_Time(s)\": [0]\n",
    "        })\n",
    "        error_results.to_csv(results_file, mode=\"a\", header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  2.  2.  3.  3.]\n",
      " [ 2.  2.  2.  3.  3.  1.  1.]\n",
      " [ 4.  4.  2.  2.  3.  3.  3.]\n",
      " [ 1.  2. nan  1.  2. nan nan]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import ClusterEnsembles as CE\n",
    "\n",
    "label1 = np.array([1, 1, 1, 2, 2, 3, 3])\n",
    "\n",
    "label2 = np.array([2, 2, 2, 3, 3, 1, 1])\n",
    "\n",
    "label3 = np.array([4, 4, 2, 2, 3, 3, 3])\n",
    "\n",
    "label4 = np.array([1, 2, np.nan, 1, 2, np.nan, np.nan]) # `np.nan`: missing value\n",
    "\n",
    "labels = np.array([label1, label2, label3, label4])\n",
    "\n",
    "# label_ce = CE.cluster_ensembles(labels)\n",
    "\n",
    "# print(label_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final material class based on majority voting: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_highest_confidence_level(material_count, materials):\n",
    "    \"\"\"\n",
    "    Returns the material class with the highest confidence level\n",
    "    based on the material count and the materials list.\n",
    "    \"\"\"\n",
    "    total_instances = sum(material_count)\n",
    "    max_count = max(material_count)\n",
    "    confidence_level = max_count / total_instances if total_instances > 0 else 0\n",
    "    material_class = materials[material_count.index(max_count)]\n",
    "    return confidence_level, material_class\n",
    "\n",
    "def confidence_level_not_unique(material_count):\n",
    "    \"\"\"\n",
    "    Checks if the confidence level is not unique.\n",
    "    \"\"\"\n",
    "    max_count = max(material_count)\n",
    "    return material_count.count(max_count) > 1\n",
    "\n",
    "def majority_voting(labels, target_confidence_level=0.8, maximum_instances=9):\n",
    "    \"\"\"\n",
    "    This is the majority voting algorithm to determine the material class.\n",
    "    Takes labels from multiple classifiers (datasets) as input.\n",
    "    \"\"\"\n",
    "    materials = [1, 2, 3, 4]\n",
    "    material_count = [0] * len(materials)\n",
    "    number_of_instances = len(labels[0])  # Assuming labels have consistent length\n",
    "    confidence_level = 0\n",
    "\n",
    "    # Voting procedure\n",
    "    for i in range(number_of_instances):\n",
    "        # Prepare a list of valid votes (ignoring np.nan)\n",
    "        votes = []\n",
    "        for label_set in labels:\n",
    "            if not np.isnan(label_set[i]):  # Ignore np.nan values\n",
    "                votes.append(label_set[i])\n",
    "        \n",
    "        if votes:\n",
    "            # Get the classification (most frequent vote)\n",
    "            material_class = np.bincount(votes).argmax()  # Majority vote\n",
    "            \n",
    "            # Increment the count for the selected material class\n",
    "            material_count[materials.index(material_class)] += 1\n",
    "\n",
    "            # Get the highest confidence level based on current material counts\n",
    "            confidence_level, material_class = get_highest_confidence_level(material_count, materials)\n",
    "\n",
    "            if confidence_level >= target_confidence_level:\n",
    "                if confidence_level_not_unique(material_count):\n",
    "                    # If confidence is not unique, pick a contender randomly\n",
    "                    material_class = random.choice(materials)\n",
    "                    break\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "    return material_class\n",
    "\n",
    "# Test labels\n",
    "label1 = np.array([1, 1, 1, 2, 2, 3, 3])\n",
    "label2 = np.array([2, 2, 2, 3, 3, 1, 1])\n",
    "label3 = np.array([4, 4, 2, 2, 3, 3, 3])\n",
    "label4 = np.array([1, 2, np.nan, 1, 2, np.nan, np.nan])  # `np.nan`: missing value\n",
    "\n",
    "# Combine the labels\n",
    "labels = np.array([label1, label2, label3, label4])\n",
    "\n",
    "# Apply majority voting to the provided labels\n",
    "final_class = majority_voting(labels)\n",
    "print(f\"Final material class based on majority voting: {final_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ijcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
